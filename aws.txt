AWS S3 Integration Code Changes

This document contains the code modifications made for integrating AWS S3 into your backend for image uploads and deletion.

---
**File: `backend/src/middlewares/upload.js`**

**Change:** Updated `multer` to use `memoryStorage()` instead of `diskStorage()` to handle file uploads in memory for serverless environments.

```javascript
const multer = require('multer');
const path = require('path');

// Set up storage engine
// Changed from diskStorage to memoryStorage for serverless environments
const storage = multer.memoryStorage();

// Check file type
function checkFileType(file, cb) {
  // Allowed ext
  const filetypes = /jpeg|jpg|png|gif/;
  // Check ext
  const extname = filetypes.test(path.extname(file.originalname).toLowerCase());
  // Check mime
  const mimetype = filetypes.test(file.mimetype);

  if (mimetype && extname) {
    return cb(null, true);
  } else {
    cb('Error: Images Only!');
  }
}

// Init upload
const upload = multer({
  storage: storage,
  limits: { fileSize: 5000000 }, // 5MB
  fileFilter: function (req, file, cb) {
    checkFileType(file, cb);
  },
}).array('images', 10);

module.exports = upload;
```

---
**File: `backend/src/middlewares/uploadPhoto.js`**

**Change:** Updated `multer` to use `memoryStorage()` instead of `diskStorage()` for single photo uploads.

```javascript
const multer = require('multer');
const path = require('path');

// Set up storage engine
// Changed from diskStorage to memoryStorage for serverless environments
const storage = multer.memoryStorage();

// Check file type
function checkFileType(file, cb) {
  // Allowed ext
  const filetypes = /jpeg|jpg|png|gif/;
  // Check ext
  const extname = filetypes.test(path.extname(file.originalname).toLowerCase());
  // Check mime
  const mimetype = filetypes.test(file.mimetype);

  if (mimetype && extname) {
    return cb(null, true);
  } else {
    cb('Error: Images Only!');
  }
}

// Init upload
const uploadPhoto = multer({
  storage: storage,
  limits: { fileSize: 5000000 }, // 5MB
  fileFilter: function (req, file, cb) {
    checkFileType(file, cb);
  },
}).single('photo');

module.exports = uploadPhoto;
```

---
**File: `backend/src/controllers/propertyController.js`**

**Change:** Integrated AWS S3 for multi-image uploads. Files are now uploaded to S3 with `public-read` ACL, and S3 URLs are saved to the database.

```javascript
const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
const crypto = require('crypto');
const Property = require('../models/Property');

const s3Client = new S3Client({
  region: process.env.AWS_REGION,
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  },
});

const generateFileName = (bytes = 32) => crypto.randomBytes(bytes).toString('hex');

// ... (other controller functions like createProperty, getProperties, getPropertyById, updateProperty, deleteProperty remain unchanged) ...

// @desc    Update property images
// @route   PUT /api/properties/:id/images
// @access  Private (Host)
exports.updatePropertyImages = async (req, res, next) => {
  try {
    let property = await Property.findById(req.params.id);

    if (!property) {
      return res.status(404).json({ success: false, message: `Property not found with id of ${req.params.id}` });
    }

    // Make sure user is property owner
    if (property.hostId.toString() !== req.user.id && req.user.role !== 'Admin') {
      return res.status(401).json({ success: false, message: 'Not authorized to update this property' });
    }

    if (req.files && req.files.length > 0) {
      const uploadPromises = req.files.map(async (file) => {
        const fileName = generateFileName();
        const params = {
          Bucket: process.env.AWS_BUCKET_NAME,
          Key: fileName,
          Body: file.buffer,
          ContentType: file.mimetype,
          ACL: 'public-read', // Ensure the uploaded file is publicly readable
        };
        const command = new PutObjectCommand(params);
        await s3Client.send(command);
        return `https://${process.env.AWS_BUCKET_NAME}.s3.${process.env.AWS_REGION}.amazonaws.com/${fileName}`;
      });

      const imageUrls = await Promise.all(uploadPromises);
      property.images = property.images.concat(imageUrls);
    } else {
      return res.status(400).json({ success: false, message: 'Please upload one or more files' });
    }

    const updatedProperty = await property.save();

    res.status(200).json({
      success: true,
      data: updatedProperty,
    });
  } catch (error) {
    next(error);
  }
};
```

---
**File: `backend/src/controllers/photoController.js`**

**Change:** Integrated AWS S3 for single photo uploads and deletion.

```javascript
const { S3Client, PutObjectCommand, DeleteObjectCommand } = require('@aws-sdk/client-s3');
const crypto = require('crypto');
const Photo = require('../models/Photo');
const asyncHandler = require('express-async-handler');

const s3Client = new S3Client({
  region: process.env.AWS_REGION,
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  },
});

const generateFileName = (bytes = 32) => crypto.randomBytes(bytes).toString('hex');


// @desc    Upload a photo
// @route   POST /api/photos
// @access  Private
exports.uploadPhoto = asyncHandler(async (req, res, next) => {
  const { caption, socialLink, property } = req.body;
  const file = req.file;

  if (!file) {
    res.status(400);
    throw new Error('Please upload a file');
  }

  const fileName = generateFileName();
  const params = {
    Bucket: process.env.AWS_BUCKET_NAME,
    Key: fileName,
    Body: file.buffer,
    ContentType: file.mimetype,
    ACL: 'public-read', // Ensure the uploaded file is publicly readable
  };

  const command = new PutObjectCommand(params);
  await s3Client.send(command);

  const imageUrl = `https://${process.env.AWS_BUCKET_NAME}.s3.${process.env.AWS_REGION}.amazonaws.com/${fileName}`;

  const photo = await Photo.create({
    user: req.user.id,
    property,
    imageUrl,
    caption,
    socialLink,
  });

  res.status(201).json({
    success: true,
    data: photo,
  });
});

// @desc    Get all photos
// @route   GET /api/photos
// @access  Public
exports.getPhotos = asyncHandler(async (req, res, next) => {
  const photos = await Photo.find().populate('user', 'name');

  res.status(200).json({
    success: true,
    count: photos.length,
    data: photos,
  });
});

// @desc    Delete a photo
// @route   DELETE /api/photos/:id
// @access  Private
exports.deletePhoto = asyncHandler(async (req, res, next) => {
  const photo = await Photo.findById(req.params.id);

  if (!photo) {
    res.status(404);
    throw new Error('Photo not found');
  }

  // Check if user is photo owner or an Admin
  if (photo.user.toString() !== req.user.id && req.user.role !== 'Admin') {
    res.status(401);
    throw new Error('Not authorized to delete this photo');
  }

  // Delete file from S3
  try {
    // Extract the key (filename) from the S3 URL
    const urlParts = photo.imageUrl.split('/');
    const key = urlParts[urlParts.length - 1]; // Assuming key is the last part of the URL
    
    const params = {
      Bucket: process.env.AWS_BUCKET_NAME,
      Key: key,
    };

    const command = new DeleteObjectCommand(params);
    await s3Client.send(command);
  } catch (error) {
    console.error('Failed to delete image from S3:', error);
    // You might want to handle this error differently, e.g., throw it or log more carefully.
    // For now, we'll continue to delete the DB record even if S3 deletion fails.
  }

  await photo.deleteOne();

  res.status(200).json({
    success: true,
    message: 'Photo removed',
  });
});
```

---
**Required Environment Variables for Vercel:**

*   `AWS_BUCKET_NAME`: The name of your S3 bucket (e.g., `homestead-images-wridip-123`).
*   `AWS_REGION`: The AWS region code where your bucket is located (e.g., `ap-southeast-2`).
*   `AWS_ACCESS_KEY_ID`: Your AWS IAM user's Access Key ID.
*   `AWS_SECRET_ACCESS_KEY`: Your AWS IAM user's Secret Access Key.

**Note:** For the bucket policy to take effect, ensure that the "Block all public access" setting for your S3 bucket is unchecked and that "ACLs enabled" is selected under Object Ownership in your S3 bucket permissions.
